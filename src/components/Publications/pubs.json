[
  {
    "year": "2020",
    "publications":[
      {
        "title": "Towards Understanding Online Question & Answer Interactions and Their Effects on Student Performance in Large-Scale STEM Classes (Accepted, June Issue)",
        "image_path": "assets/qa-cloud.png",
        "link": "",
        "abstract": "Online question & answer (Q & A) is a distinctive type of online interaction that is impactful on student learning. Prior studies on online interaction in large-scale classes mainly focused on online discussion and were conducted mainly in non-STEM fields. This research aims to quantify the effects of online Q & A interactions on student performance in the context of STEM education. 218 computer science students from a large university in the southeastern United States participated in this research. Data of four online Q & A activities was mined from the online Q & A forum for the course, including three student activities (asking questions, answering questions and viewing questions/answers) and one instructor activity (answering questions/providing clarifications). These activities were found to have different effects on student performance. Viewing questions/answers was found to have the greatest effect, while interaction with instructors showed minimum effects. This research fills the gap of lacking research in online Q & A, and the results of this research can inform the effective usage of online Q & A in large-scale STEM courses."
      }
    ]
  },
  {
    "year": "2019",
    "publications": [
      {
        "title": "Quantifying the Effects of Prior Knowledge in Entry-Level Programming Courses",
        "image_path": "assets/comped.png",
        "link": "https://dl.acm.org/citation.cfm?id=3309503&preflayout=tabs" ,
        "abstract": "Computer literacy and programming are being taught increasingly at the K-12 level with more students than ever matriculating in college with prior programming experience. Accurately assessing student programming skills acquired in high school can inform college faculty about the range of competencies in introductory programming courses. The tool predominantly-used for assessing past CS knowledge and skills is a survey, which lacks quantitative rigor. This study aims to (1) quantify the effects of prior knowledge in entry-level programming courses and (2) compare the different measurement approaches of student prior knowledge in programming, including surveys and aptitude tests. The results of this study reveal that a discrepancy exists between the results of surveys and aptitude tests. Consistent with prior survey studies, our survey results showed that the effects of student prior programming knowledge faded gradually during the course period. In contrast, the aptitude test results indicated that the effects of student prior knowledge did not weaken over time. The accuracy of both measurements and implications for instructors were further discussed."
      },
      {
        "title": "A Systematic Investigation of Replications in Computing Education Research",
        "image_path": "assets/meta.png",
        "link": "https://dl.acm.org/doi/10.1145/3345328",
        "abstract": "As the societal demands for application and knowledge in computer science increase, computer science student enrollment keeps growing rapidly around the world. By continuously improving the efficacy of computing education and providing guidelines for learning and teaching practice, computing education research plays a vital role in addressing both educational and societal challenges that emerge from the growth of computer science students. Given the significant role of computing education research, it is important to ensure the reliability of studies in this field. The extent to which studies can be replicated in a field is one of the most important standards for reliability. Different fields have paid an increasing attention to the replication rates of their studies, but the replication rate of computing education was never systematically studied. To fill this gap, this study investigated the replication rate of computing education between 2009 and 2018. We examined 2,269 published studies from three major conferences and two major journals in computing education, and found that the overall replication rate of computing education was 2.38%. This study demonstrated the needs for more replication studies in computing education and discussed how to encourage replication studies through research initiatives and policy making."
      },
      {
        "title": "Investigating the Essential of Meaningful Automated Formative Feedback for Programming Assignments",
        "image_path": "assets/feedback.png",
        "link": "https://arxiv.org/abs/1906.08937",
        "abstract": "This study investigated the essential of meaningful automated feedback for programming assignments. Three different types of feedback were tested, including (a) What's wrong - what test cases were testing and which failed, (b) Gap - comparisons between expected and actual outputs, and (c) Hint - hints on how to fix problems if test cases failed. 46 students taking a CS2 participated in this study. They were divided into three groups, and the feedback configurations for each group were different: (1) Group One - What's wrong, (2) Group Two - What's wrong + Gap, (3) Group Three - What's wrong + Gap + Hint. This study found that simply knowing what failed did not help students sufficiently, and might stimulate system gaming behavior. Hints were not found to be impactful on student performance or their usage of automated feedback. Based on the findings, this study provides practical guidance on the design of automated feedback."
      }
    ]
  }

]
